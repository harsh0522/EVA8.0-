{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1uJZvJdi5VprOQHROtJIHy0mnY2afjNlx","timestamp":1670529825285}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"ZYaImUXrI7iw"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0m2JWFliFfKT","executionInfo":{"status":"ok","timestamp":1670546079740,"user_tz":-330,"elapsed":3154,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"source":["from __future__ import print_function #__future__ module is a built-in module in Python that is used to inherit new features that will be available in the new Python versions, from that it is importing module called print_function\n","import torch #PyTorch is a Python package that provides two high-level features: 1- Tensor computation (like NumPy) with strong GPU acceleration 2- Deep neural networks built on a tape-based autograd system\n","import torch.nn as nn #PyTorch provides the elegantly designed modules and classes torch.nn , torch.optim , Dataset , and DataLoader to help you create and train neural networks, \n","                      #here (as nn) basically it is convention or short form use for torch.nn\n","import torch.nn.functional as F #in torch.nn there is a module called functional so we are calling that as F(which basically short form so we dont have to write complete 2 words again and again)  \n","import torch.optim as optim #torch.optim basically this package has various optimization algorithms example optim.SGD, optim.Adam\n","from torchvision import datasets, transforms #torchvision datasets for various videos and images, it is opensource"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_Cx9q2QFgM7","executionInfo":{"status":"ok","timestamp":1670549200273,"user_tz":-330,"elapsed":1,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"source":["#so here our NN is created\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input 30x30 OUtput 28x28 RF 3x3  \n","                                                    #nn.Conv2d(1, 32, 3, padding=1) here I/P image is using 3x3 matrix/kernel and has 32 channels hence 32 kernels so like this maybe\n","                                                    # 30x30x1 | (3x3)x32 | 28x28x32 \n","\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) #input 30x30 OUtput 28x28 RF 5x5  ## 30x30x32 | (3x3)x64 | 28x28x64 \n","        self.pool1 = nn.MaxPool2d(2, 2) #input 28x28 OUtput 14x14 RF 10x10\n","        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)#input 16x16 OUtput 14x14 RF 12x12 ##16x16x64 | (3x3)x128 | 14x14x128 \n","        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)#input 16x16 OUtput 14x14 RF 14x14 ##16x16x128 | (3x3)x256 | 14x14x256\n","        self.pool2 = nn.MaxPool2d(2, 2) #input 14x14 OUtput 7x7 RF 28x28 \n","        self.conv5 = nn.Conv2d(256, 512, 3) #input 7x7 OUtput 5x5 RF 30x30 ##7x7x256 | (3x3)x512 | 14x14x512\n","        self.conv6 = nn.Conv2d(512, 1024, 3) #input 5x5 OUtput 3x3 RF 32x32 ##5x5x512 | (3x3)x1024 | 3x3x1024\n","        self.conv7 = nn.Conv2d(1024, 80, 3) #input 3x3 OUtput 1x1 RF 34x34 ##3x3x1024 | (3x3)x10 ? why we are using only 10 kernels of 3x3 strange let me ask sir | 1x1x10 \n","\n","    def forward(self, x):\n","        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) #Network is created with activation function relu\n","        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x))))) #Network is created with activation function relu\n","        x = F.relu(self.conv6(F.relu(self.conv5(x)))) #same relu function is applied in conv 5,6 and 7\n","        x = F.relu(self.conv7(x))\n","        x = x.view(-1, 80)\n","        return F.log_softmax(x)#I think in this we have use log to fall values between 0 and 1 or may be log probabilities"],"execution_count":104,"outputs":[]},{"cell_type":"code","metadata":{"id":"xdydjYTZFyi3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670549206406,"user_tz":-330,"elapsed":2730,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}},"outputId":"582e4b6e-83de-47af-f56e-74e3f302c850"},"source":["!pip install torchsummary #to install any module in python we use pip\n","from torchsummary import summary #this module provide summary of a model that we made #summary(model, input_size=(1, 28, 28)) this one which provide brief summary about our model\n","use_cuda = torch.cuda.is_available()#Returns a bool(means true or false) indicating if CUDA is currently available or not\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\") #if CUDA is not available use CPU\n","model = Net().to(device)#object will internally move all parameters and buffers to the device, dtype, or memory format so you wouldnâ€™t need to reassign a model:\n","summary(model, input_size=(1, 28, 28))#provide summary of model"],"execution_count":105,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchsummary in /usr/local/lib/python3.8/dist-packages (1.5.1)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 32, 28, 28]             320\n","            Conv2d-2           [-1, 64, 28, 28]          18,496\n","         MaxPool2d-3           [-1, 64, 14, 14]               0\n","            Conv2d-4          [-1, 128, 14, 14]          73,856\n","            Conv2d-5          [-1, 256, 14, 14]         295,168\n","         MaxPool2d-6            [-1, 256, 7, 7]               0\n","            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n","            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n","            Conv2d-9             [-1, 80, 1, 1]         737,360\n","================================================================\n","Total params: 7,024,976\n","Trainable params: 7,024,976\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 1.51\n","Params size (MB): 26.80\n","Estimated Total Size (MB): 28.31\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-104-6762c46d0586>:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)#I think in this we have use log to fall values between 0 and 1 or may be log probabilities\n"]}]},{"cell_type":"code","metadata":{"id":"DqTWLaM5GHgH","executionInfo":{"status":"ok","timestamp":1670549209765,"user_tz":-330,"elapsed":2,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"source":["\n","\n","torch.manual_seed(1)#basically used for generating random numbers\n","batch_size = 128 #image will be divided in batch size or maybe image will be processed in batch\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n"],"execution_count":106,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fDefDhaFlwH","executionInfo":{"status":"ok","timestamp":1670549212662,"user_tz":-330,"elapsed":1,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"source":["from tqdm import tqdm #provides progress bar while training\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()#now we will train our model using train function/module\n","    pbar = tqdm(train_loader)\n","    for batch_idx, (data, target) in enumerate(pbar):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n","\n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":107,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMWbLWO6FuHb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670549239214,"user_tz":-330,"elapsed":23912,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}},"outputId":"4ff91e90-f853-4a08-dc53-ef9c3753b8f0"},"source":["\n","model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)#how we are deciding learning rate here as 0.01 #what is momentum here?\n","\n","for epoch in range(1, 2):\n","    train(model, device, train_loader, optimizer, epoch)#train function which has many parameters like model, device, train loader etc\n","    test(model, device, test_loader)#now testing our model on unseen data basically"],"execution_count":108,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/469 [00:00<?, ?it/s]<ipython-input-104-6762c46d0586>:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.log_softmax(x)#I think in this we have use log to fall values between 0 and 1 or may be log probabilities\n","loss=0.17707271873950958 batch_id=468: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 469/469 [00:21<00:00, 21.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Test set: Average loss: 0.0720, Accuracy: 9779/10000 (98%)\n","\n"]}]},{"cell_type":"code","metadata":{"id":"So5uk4EkHW6R","executionInfo":{"status":"ok","timestamp":1670548569370,"user_tz":-330,"elapsed":2,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"source":["#self.conv7 = nn.Conv2d(1024, 80, 3)\n","#x = x.view(-1, 80)\n","#Test set: Average loss: 0.0680, Accuracy: 9796/10000 (98%)"],"execution_count":69,"outputs":[]},{"cell_type":"code","source":["#self.conv7 = nn.Conv2d(1024, 80, 3)\n","#x = x.view(-1, 80)\n","#Test set: Average loss: 0.0680, Accuracy: 9796/10000 (98%)\n","#Test set: Average loss: 0.0539, Accuracy: 9825/10000 (98%)\n","#strange accuracy is same but average loss is reduced"],"metadata":{"id":"01Gs24VibgzS","executionInfo":{"status":"ok","timestamp":1670548571776,"user_tz":-330,"elapsed":1,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["#self.conv7 = nn.Conv2d(1024, 240, 3)\n","#x = x.view(-1, 240)\n","#Test set: Average loss: 0.0512, Accuracy: 9831/10000 (98%)\n","#accuracy is same but loss is decreasing"],"metadata":{"id":"F4Cii0npb-w4","executionInfo":{"status":"ok","timestamp":1670548573794,"user_tz":-330,"elapsed":1,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":71,"outputs":[]},{"cell_type":"code","source":["#lol is increased batch size and accuracy decreases\n","#  0%|          | 0/235 [00:00<?, ?it/s]<ipython-input-61-c51a7e299078>:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"," # return F.log_softmax(x)#I think in this we have use log to fall values between 0 and 1 or may be log probabilities\n","#loss=0.11128437519073486 batch_id=234: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 235/235 [00:16<00:00, 14.35it/s]\n","\n","#Test set: Average loss: 0.0897, Accuracy: 9716/10000 (97%)"],"metadata":{"id":"Zsz6KuBlc54W","executionInfo":{"status":"ok","timestamp":1670548575767,"user_tz":-330,"elapsed":2,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["#   0%|          | 0/938 [00:00<?, ?it/s]<ipython-input-61-c51a7e299078>:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","#   return F.log_softmax(x)#I think in this we have use log to fall values between 0 and 1 or may be log probabilities\n","# loss=0.028453771024942398 batch_id=937: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 938/938 [00:22<00:00, 40.80it/s]\n","\n","# Test set: Average loss: 0.1161, Accuracy: 9637/10000 (96%)\n","\n","\n","#again starange a reduced batch size to 64 and accuracy decreased"],"metadata":{"id":"wDgUBocQdTV3","executionInfo":{"status":"ok","timestamp":1670548669064,"user_tz":-330,"elapsed":597,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["#even after this self.conv7 = nn.Conv2d(1024, 40, 3) accuracy not incresed only slight changes in loss"],"metadata":{"id":"VTmVZ8ORdceg","executionInfo":{"status":"ok","timestamp":1670548946466,"user_tz":-330,"elapsed":569,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":95,"outputs":[]},{"cell_type":"code","source":["#even after this self.conv7 = nn.Conv2d(1024, 60, 3) accuracy 97%"],"metadata":{"id":"CsoLMxafehK8","executionInfo":{"status":"ok","timestamp":1670548979408,"user_tz":-330,"elapsed":553,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":96,"outputs":[]},{"cell_type":"code","source":["#even after this self.conv7 = nn.Conv2d(1024, 60, 3) accuracy 97%\n"],"metadata":{"id":"7ESozOspepP8","executionInfo":{"status":"ok","timestamp":1670549340105,"user_tz":-330,"elapsed":2,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":110,"outputs":[]},{"cell_type":"code","source":["#even after this self.conv7 = nn.Conv2d(1024, 80, 3) accuracy 98% \n","#after 80 accuracy is not increasing above 98 percent only loss in decreasing"],"metadata":{"id":"8Fk0GVWDfw6j","executionInfo":{"status":"ok","timestamp":1670549338246,"user_tz":-330,"elapsed":2,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":109,"outputs":[]},{"cell_type":"code","source":["#maybe real enginnering a nural network lies to increase accuracy above 98% hahahahahahhahahahahah"],"metadata":{"id":"C8VIPay0gmPQ","executionInfo":{"status":"ok","timestamp":1670549561536,"user_tz":-330,"elapsed":824,"user":{"displayName":"harsh agarwal","userId":"02583193928313641908"}}},"execution_count":112,"outputs":[]}]}